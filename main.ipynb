{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc0952f",
   "metadata": {},
   "source": [
    "# AIS Port Classification notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d874d",
   "metadata": {},
   "source": [
    "This notebook is a walktrough of the AIS Marinetime Project by group 135. In this project, we have focused on Port Classification given time series data. \n",
    "\n",
    "The notebook consits of the following contents:\n",
    "- [Introduction](#introduction-to-methods)\n",
    "- [Setup](#setup) \n",
    "- [Data](#Data-exploration) \n",
    "    - [Data source & preparation](#data-sources--ingestion) \n",
    "    - [Descriptive statistics & Data Visualization](#descriptive-statistics) \n",
    "    - [Feature Engineering](#feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898891c7",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf3db1",
   "metadata": {},
   "source": [
    "In this section, we will introduce the goal and methods in this project. We will present the different models and techniques used for port classification using AIS data. \n",
    "\n",
    "## The Goal of the Project \n",
    "The goal of this project is to build a model that can accurately classify ports based on AIS data. \n",
    "To achive this, we will have used 3 deep learning models and one simple machine learning model as a baseline. The models used are:  \n",
    "1. LSTM - Long Short-Term Memory network \n",
    "2. Transformer\n",
    "3. LSTM-Transformer Hybrid Model\n",
    "4. \"TO DO \" (baseline) \n",
    "\n",
    "\n",
    "For each model, the output is from a feed forward neural network with can be toughts of as logits. \n",
    "\n",
    "We use the cross-entropy loss function to train the models, as it is well suited for multi-class classification problems. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be27346",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38ec92",
   "metadata": {},
   "source": [
    "Before running this notebook, make sure to install the required dependencies listed in the `pyproject.toml` file and have access to the dataset used in this project.\n",
    "\n",
    "This project uses `uv` as the Python package manager. All dependencies are defined in `pyproject.toml`.\n",
    "\n",
    "Run the following commands to set up your environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42a15b",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Install all dependencies from pyproject.toml\n",
    "uv sync\n",
    "\n",
    "# Install the project in editable mode (to import from src/)\n",
    "uv pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac3978",
   "metadata": {},
   "source": [
    "The cleaned dataset used in this project can be found here: [https://drive.google.com/drive/folders/1o17rqpOwHRdrgkoRS8rd5g62xi9nsTuk?hl=da] \n",
    "\n",
    "The folder is called \"processed_data\" and should be zipped and placed in the data directory of this project.\n",
    "\n",
    "Otherwise, you can down the data in the next cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "882f0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from Dataloader import Dataloader \n",
    "import numpy as np \n",
    "import torch  \n",
    "from RNN import clean_data, setup_and_train\n",
    "from utils import MaritimeDataset, plot_trajectory_on_map, download\n",
    "import os\n",
    "import zipfile \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import mlflow \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2192ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"TRAIN_OWN_MODEL\": True, \n",
    "\n",
    "\n",
    "    \"DOWNLOAD_DATA_FROM_SOURCE\": False, #Set to True if data needs to be downloaded from source\n",
    "    \"Experiment_Name\": \"Test Notebook\",\n",
    "    #\"Experiment_Name\": \"Classifier-Experiment - Full Run 1: 25.Nov.2025\", \n",
    "\n",
    "\n",
    "    ##Paths for saving and loading models and data\n",
    "    \"LOAD_CLEANED_DATA_PATH\": \"data/cleaned_data.parquet\", \n",
    "    \"DATA_PATH\": \"data/processed_data\", \n",
    "}\n",
    "\n",
    "\n",
    "#By chaning the hyperparams, it should be noted that no Pretrained models exist\n",
    "hyperparams = {\n",
    "        \"general\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"num_epochs\": 50,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        },\n",
    "\n",
    "        #Model specific hyperparameters\n",
    "        \"LSTM\": {\n",
    "        \"lstm_hidden_size\": 64,\n",
    "        \"lstm_num_layers\": 5,\n",
    "        \"dropout\": 0.1,\n",
    "        \"batch_first\": True\n",
    "        }, \n",
    "\n",
    "        \"LSTM_Transformer\": {\n",
    "        \"hidden_size\": 256,\n",
    "        \"num_lstm_layers\": 5,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_transformer_layers\": 6,\n",
    "        \"dropout\": 0.1,\n",
    "        \"batch_first\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "#Setting seed for reproducibility \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.cuda.manual_seed_all(42)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546f86",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a5c47",
   "metadata": {},
   "source": [
    "Downloading data from source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16404806",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config[\"DOWNLOAD_DATA_FROM_SOURCE\"]):    \n",
    "    data_links = {\"Jan\": \"http://aisdata.ais.dk/2023/aisdk-2023-01.zip\",\n",
    "                  \"Feb\": \"http://aisdata.ais.dk/2023/aisdk-2023-02.zip\",\n",
    "                  \"Mar\": \"http://aisdata.ais.dk/2023/aisdk-2023-03.zip\",\n",
    "                  \"Apr\": \"http://aisdata.ais.dk/2023/aisdk-2023-04.zip\",\n",
    "                  \"May\": \"http://aisdata.ais.dk/2023/aisdk-2023-05.zip\",\n",
    "                  \"Jun\": \"http://aisdata.ais.dk/2023/aisdk-2023-06.zip\",\n",
    "                  \"Jul\": \"http://aisdata.ais.dk/2023/aisdk-2023-07.zip\",\n",
    "                  \"Aug\": \"http://aisdata.ais.dk/2023/aisdk-2023-08.zip\",\n",
    "                  \"Sep\": \"http://aisdata.ais.dk/2023/aisdk-2023-09.zip\",\n",
    "                  \"Oct\": \"http://aisdata.ais.dk/2023/aisdk-2023-10.zip\",\n",
    "                  \"Nov\": \"http://aisdata.ais.dk/2023/aisdk-2023-11.zip\",\n",
    "                  \"Dec\": \"http://aisdata.ais.dk/2023/aisdk-2023-12.zip\"\n",
    "                 }\n",
    "\n",
    "    data_dir = \"data/unprocessed\"\n",
    "    end_dir = \"data/processed\"\n",
    "\n",
    "\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(end_dir, exist_ok=True)\n",
    "\n",
    "    for month, link in data_links.items():\n",
    "        filename = link.split(\"/\")[-1]\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "        # Extract month number from filename\n",
    "        month_num = filename.split(\"-\")[2].replace(\".zip\", \"\")\n",
    "\n",
    "        # Check if all parquet files for this month already exist\n",
    "        # Quick check: if folder has files matching this month pattern\n",
    "        existing_parquets = [f for f in os.listdir(end_dir) if f.startswith(f\"aisdk-2023-{month_num}-\") and f.endswith('.parquet')]\n",
    "\n",
    "        # A month should have 28-31 parquet files\n",
    "        if len(existing_parquets) >= 28:\n",
    "            print(f\"‚è≠Ô∏è  Skipping month {month}: {len(existing_parquets)} parquet files already exist\")\n",
    "            continue\n",
    "\n",
    "        # Download if needed\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"‚è≠Ô∏è  Skipping download for month {month}: {filepath} (already exists)\")\n",
    "        else:\n",
    "            print(f\"Downloading data for {month}...\")\n",
    "            download(link, filepath)\n",
    "            print(f\"Downloaded data for {month}\")\n",
    "\n",
    "        # Open ZIP once and process all CSV files\n",
    "        with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "            csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "            print(f\"Found {len(csv_files)} CSV files in {filename}\")\n",
    "\n",
    "            for csv_filename in csv_files:\n",
    "                output_filename = csv_filename.replace('.csv', '.parquet')\n",
    "                output_path = os.path.join(end_dir, output_filename)\n",
    "\n",
    "                if os.path.exists(output_path):\n",
    "                    print(f\"‚è≠Ô∏è  Skipping {output_filename} (already exists)\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"üìù Processing: {csv_filename}\")\n",
    "\n",
    "                data = Dataloader(\n",
    "                    file_path=\"\",\n",
    "                    out_path=output_path,\n",
    "                    zip_path=filepath,\n",
    "                    csv_internal_path=csv_filename\n",
    "                )\n",
    "                data.clean_data()\n",
    "\n",
    "        print(f\"‚úÖ Processed {month}\")\n",
    "\n",
    "        # Remove ZIP after processing \n",
    "        os.remove(filepath)\n",
    "        print(f\"üóëÔ∏è  Removed {filename}\")\n",
    "\n",
    "    print(\"üéâ All data processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb4361",
   "metadata": {},
   "source": [
    "## Data source & Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbc5dc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "All the data is being prerocessed and cleaned in the prepare_data.ipynb notebook.\n",
    "\n",
    "We do the following steps in the data preparation:\n",
    "1. Download 2023 raw data from [AIS data source](http://aisdata.ais.dk/)\n",
    "2. We only keep the column that we deem are relevant for our project i.e \n",
    "    - MMSI\n",
    "    - SOG \n",
    "    - COG\n",
    "    - Longitude\n",
    "    - Latitude\n",
    "    - Timestamp \n",
    "    - Ship type \n",
    "    - Type of mobile  \n",
    "    - Length \n",
    "    - Width \n",
    "    - Destination \n",
    "\n",
    "3. We filter the following data points: \n",
    "    - data points not in the Danish waters. \n",
    "    - data points which are classfied as \"Ship type A\" and then remove all other ship types. (Type of mobile is redundant after this step and removed).  \n",
    "    - 1 <= SOG <= 50 knots \n",
    "    - Sequences below 250 data points are removed. \n",
    "    - data with non-valid port codes are removed (/data_port_locodes.csv are used a source of truth)\n",
    "    - Filter anomalities where the contains jump in data points that are not realistic (based on SOG and distance between points). \n",
    "\n",
    "\n",
    "4. We do the following transformations:\n",
    "    - Label the data points for each date into segments\n",
    "    - Datapoints are concated based on segments such that each sequence of a ship can stretch over multiple days. \n",
    "    - downsampling such that there is at least 10 seconds between each data point (if two points are closer than 10 seconds, we remove the first point). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "179e491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config[\"DOWNLOAD_DATA_FROM_SOURCE\"]):\n",
    "    path = config[\"DATA_PATH\"]\n",
    "    dataloader = Dataloader(out_path=path)\n",
    "    df = dataloader.load_data()  # load all files in the processed_data folder\n",
    "    #Same cleaning as in RNN \n",
    "    df_clean = clean_data(df) \n",
    "    df_clean.to_parquet(config[\"LOAD_CLEANED_DATA_PATH\"], index=False)  \n",
    "\n",
    "else: \n",
    "    df_clean = pd.read_parquet(config[\"LOAD_CLEANED_DATA_PATH\"]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085445b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883bf86",
   "metadata": {},
   "source": [
    "## Descriptive statistics & Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c1de1",
   "metadata": {},
   "source": [
    "First we check how the distribution of the different ports are in the cleaned dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db168cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = df_clean['Port'] \n",
    "ports_norm = ports.value_counts(normalize=True)  # Count occurrences of each port\n",
    "\n",
    "# Plot top 10 ports as bar chart\n",
    "top_10 = ports_norm.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10.plot(kind='bar')\n",
    "plt.xlabel('Port', fontsize=12)\n",
    "plt.ylabel('Proportion', fontsize=12)\n",
    "plt.title('Top 10 Most Frequent Ports (Normalized)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33898e13",
   "metadata": {},
   "source": [
    "Because we see that there is a lot of inbalance between the groups, we will need to balance this out succh that our model doesnt just predict the \"no_port\" class. \n",
    "\n",
    "To solve this problem, we will introduce class weights. We use skilearn's `compute_class_weight` function to calculate the class weights based on the frequency of each class in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70400190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_trajectory_on_map(df_clean, percentage_of_vessels=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e55d5f",
   "metadata": {},
   "source": [
    "To check if seasonality plays a role in the data, we plot the weekly observations for each port type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca045acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekly_observations(df, timestamp_col=\"Timestamp\"):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[timestamp_col] = pd.to_datetime(df_copy[timestamp_col])\n",
    "    \n",
    "\n",
    "    df_copy['YearWeek'] = df_copy[timestamp_col].dt.to_period('W')\n",
    "    weekly_counts = df_copy['YearWeek'].value_counts().sort_index()\n",
    "    \n",
    "    weekly_counts.index = weekly_counts.index.to_timestamp()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(weekly_counts.index, weekly_counts.values, marker='o', linewidth=1.5, markersize=4)\n",
    "    plt.xlabel('Week', fontsize=12)\n",
    "    plt.ylabel('Number of Observations', fontsize=12)\n",
    "    plt.title('Weekly AIS Observations Over Time', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Total weeks: {len(weekly_counts)}\")\n",
    "    print(f\"Total observations: {weekly_counts.sum():,}\")\n",
    "    print(f\"Average observations per week: {weekly_counts.mean():.0f}\")\n",
    "    print(f\"Min observations in a week: {weekly_counts.min():,}\")\n",
    "    print(f\"Max observations in a week: {weekly_counts.max():,}\")\n",
    "    \n",
    "    return weekly_counts\n",
    "\n",
    "\n",
    "weekly_obs = plot_weekly_observations(df_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d11cf",
   "metadata": {},
   "source": [
    "It looks like there is some seasonality in the data, with peaks in the summer months and troughs in the winter months. To invistiage this further and if it have a impact on our ports, we plot the ports observations over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af112c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for port_type in top_10.index:\n",
    "    print(f\"Plotting weekly observations for ship type: {port_type}\") \n",
    "    df_type = df_clean[df_clean['Port'] == port_type]\n",
    "    plot_weekly_observations(df_type) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57751142",
   "metadata": {},
   "source": [
    "Its now very clear that there is seasonality in the data for each port.  \n",
    "\n",
    "To adress this, we wil not use absolute timestamps as input features to our models. Instead, we will use relative time features. This way, the model can learn to generalize across different time periods without being affected by seasonality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b423d",
   "metadata": {},
   "source": [
    "We now look at the distribution of the ship types in the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74447732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_types = df_clean['Ship type'] \n",
    "ship_types_norm = ship_types.value_counts(normalize=True)  # Count occurrences of each ship type\n",
    "\n",
    "# Plot top 10 ship types as bar chart\n",
    "top_10 = ship_types_norm.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10.plot(kind='bar')\n",
    "plt.xlabel('Ship Type', fontsize=12)\n",
    "plt.ylabel('Proportion', fontsize=12)\n",
    "plt.title('Top 10 Most Frequent Ship Types (Normalized)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6f1a8",
   "metadata": {},
   "source": [
    "We see that the majority of the ships are cargo ships, followed by tankers and passenger ships. It will be considered in future work to feature engineer the data based on ship type to see if this improves the model performance. But for this project, we will not consider ship type as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3864a",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec0d2b",
   "metadata": {},
   "source": [
    "To make sure we consider features that can help the model perform better, we do a PCA analysis to see which features are the most important. \n",
    "\n",
    "We have already done a few feature engineering steps in the data preparation notebook, such as: \n",
    "- Finding cos & sin of COG to represent direction better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = dataloader.train_test_split(df = df_clean, prediction_horizon_hours=2.0)\n",
    "train_df, val_df = dataloader.train_test_split(df = train_df, prediction_horizon_hours=0, test_size=0.2) # should not remove further hours for validation as 2 hours are already removed\n",
    "\n",
    "train_dataset = MaritimeDataset(train_df)\n",
    "print(\"Feature Engineering: New columns in input data are\")\n",
    "train_dataset.feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f036744",
   "metadata": {},
   "source": [
    "We now do a PCA analysis on our train set to wich features are the most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af97c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensor_list = train_dataset.sequences\n",
    "\n",
    "k = len(train_dataset.feature_cols) \n",
    "\n",
    "# Concatenate all tensors\n",
    "all_data = torch.cat(tensor_list, dim=0).numpy()  # Shape: (total_timesteps, n_features)\n",
    "\n",
    "# Fit PCA once on all data\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "all_data_scaled = scaler.fit_transform(all_data)\n",
    "\n",
    "pca.fit(all_data_scaled)\n",
    "\n",
    "# Explained variance ratio for each component\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(explained_var)), explained_var)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Variance Explained by Each Component')\n",
    "plt.show()\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_var, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8a66c",
   "metadata": {},
   "source": [
    "We see that no feature explain under 10% of the variance, which means that all features are important for the model and should be kept. The high variance per feature also indicates that there is low multicollinearity between the features, which is a good sign for the model performance. \n",
    "\n",
    "To check for multicollinearity between the features, we also plot a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = pd.DataFrame(all_data_scaled, columns=train_dataset.feature_cols).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_pairs.append((i, j, corr_matrix.iloc[i, j]))\n",
    "\n",
    "print(\"Highly correlated feature pairs:\")\n",
    "for i, j, corr in high_corr_pairs:\n",
    "    print(f\"Feature {i} & Feature {j}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c48968",
   "metadata": {},
   "source": [
    "We see that there a no highly correlated features (correlation > 0.8), which indicates low multicollinearity between the features. Therefore, the data for the model is clean and ready for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b1d3",
   "metadata": {},
   "source": [
    "We now train (or load pretrained) models. \n",
    "\n",
    "For the full training details, see the **RNN.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = [\"LSTM\", \"LSTM_Transformer\"] \n",
    "\n",
    "client = mlflow.MlflowClient() \n",
    "\n",
    "experiment = client.get_experiment_by_name(config[\"Experiment_Name\"]) \n",
    "\n",
    "if (experiment is None and config[\"TRAIN_OWN_MODEL\"]):\n",
    "    experiment_id = client.create_experiment(config[\"Experiment_Name\"])\n",
    "\n",
    "elif (experiment is None and not config[\"TRAIN_OWN_MODEL\"]):\n",
    "    raise ValueError(f\"Experiment '{config['Experiment_Name']}' does not exist. Cannot load pretrained model.\")  \n",
    "\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde4dbb",
   "metadata": {},
   "source": [
    "Training own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db47cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: LSTM\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "Grouping data into sequences...\n",
      "Grouping data into sequences...\n",
      "Created 6663 sequences.\n",
      "Class weights calculated: tensor([6.6630e+01, 8.3287e+00, 5.2055e-01, 1.3326e+01, 8.3287e+00, 6.6630e+01,\n",
      "        3.5068e+00, 3.3315e+01, 1.6657e+01, 4.9356e-01, 4.1644e+00, 4.4420e+00,\n",
      "        1.6657e+01, 6.6630e+01, 3.4885e-01, 3.3315e+01, 1.1105e+01, 3.0286e+00,\n",
      "        2.2210e+01, 9.5186e+00, 2.2210e+00, 1.4485e+00, 8.7671e-01, 6.0573e+00,\n",
      "        9.5186e+00, 1.6657e+01, 6.6630e+01, 4.8635e-01, 8.1256e-01, 7.4033e+00,\n",
      "        3.3315e+01, 2.7762e+00, 2.0822e+00, 1.6657e+01, 1.0576e+00, 6.0573e-01,\n",
      "        9.5186e+00, 1.6657e+01, 1.9597e+00, 8.3287e+00, 2.2210e+01, 5.5525e+00,\n",
      "        3.3315e+01, 6.6630e+01, 2.3796e+00, 3.5068e+00, 3.7017e+00, 1.0747e+00,\n",
      "        1.2813e+00, 2.4678e+00, 3.9194e+00, 9.5186e+00, 6.6630e+00, 9.5186e+00,\n",
      "        3.3315e+00, 2.2210e+01, 1.6657e+01, 3.3315e+01, 3.3315e+01, 9.5186e+00,\n",
      "        3.1729e+00, 3.3315e+01, 3.1729e+00, 3.0286e+00, 2.2210e+01, 3.3315e+00,\n",
      "        6.6630e+01, 1.3326e+01, 2.7762e+00, 6.6630e+01, 1.1105e+01, 1.6658e+00,\n",
      "        3.3315e+01, 2.2976e+00, 2.8970e+00, 5.6466e-01, 6.6630e+01, 6.6630e+01,\n",
      "        1.6657e+01, 3.3315e+01, 6.6630e+01, 6.6630e+00, 9.5186e+00, 3.3315e+01,\n",
      "        6.6630e+01, 2.4678e+00, 4.7593e-01, 2.0822e+00, 7.2424e-01, 3.3315e+01,\n",
      "        8.3287e+00, 1.6251e+00, 9.5186e-01, 6.6630e+01, 2.2210e+01, 3.3315e+01,\n",
      "        4.7593e+00, 1.2813e+00, 1.3326e+01, 1.5733e-02])\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "Created 6663 sequences.\n",
      "Class weights calculated: tensor([6.6630e+01, 8.3287e+00, 5.2055e-01, 1.3326e+01, 8.3287e+00, 6.6630e+01,\n",
      "        3.5068e+00, 3.3315e+01, 1.6657e+01, 4.9356e-01, 4.1644e+00, 4.4420e+00,\n",
      "        1.6657e+01, 6.6630e+01, 3.4885e-01, 3.3315e+01, 1.1105e+01, 3.0286e+00,\n",
      "        2.2210e+01, 9.5186e+00, 2.2210e+00, 1.4485e+00, 8.7671e-01, 6.0573e+00,\n",
      "        9.5186e+00, 1.6657e+01, 6.6630e+01, 4.8635e-01, 8.1256e-01, 7.4033e+00,\n",
      "        3.3315e+01, 2.7762e+00, 2.0822e+00, 1.6657e+01, 1.0576e+00, 6.0573e-01,\n",
      "        9.5186e+00, 1.6657e+01, 1.9597e+00, 8.3287e+00, 2.2210e+01, 5.5525e+00,\n",
      "        3.3315e+01, 6.6630e+01, 2.3796e+00, 3.5068e+00, 3.7017e+00, 1.0747e+00,\n",
      "        1.2813e+00, 2.4678e+00, 3.9194e+00, 9.5186e+00, 6.6630e+00, 9.5186e+00,\n",
      "        3.3315e+00, 2.2210e+01, 1.6657e+01, 3.3315e+01, 3.3315e+01, 9.5186e+00,\n",
      "        3.1729e+00, 3.3315e+01, 3.1729e+00, 3.0286e+00, 2.2210e+01, 3.3315e+00,\n",
      "        6.6630e+01, 1.3326e+01, 2.7762e+00, 6.6630e+01, 1.1105e+01, 1.6658e+00,\n",
      "        3.3315e+01, 2.2976e+00, 2.8970e+00, 5.6466e-01, 6.6630e+01, 6.6630e+01,\n",
      "        1.6657e+01, 3.3315e+01, 6.6630e+01, 6.6630e+00, 9.5186e+00, 3.3315e+01,\n",
      "        6.6630e+01, 2.4678e+00, 4.7593e-01, 2.0822e+00, 7.2424e-01, 3.3315e+01,\n",
      "        8.3287e+00, 1.6251e+00, 9.5186e-01, 6.6630e+01, 2.2210e+01, 3.3315e+01,\n",
      "        4.7593e+00, 1.2813e+00, 1.3326e+01, 1.5733e-02])\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "Grouping data into sequences...\n",
      "Grouping data into sequences...\n",
      "Created 2072 sequences.\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "Created 2072 sequences.\n",
      "The 'no_port' ID is successfully set to: 99\n",
      "Grouping data into sequences...\n",
      "Grouping data into sequences...\n",
      "Created 1660 sequences.\n",
      "Training on mps\n",
      "Using weighted CrossEntropyLoss\n",
      "Created 1660 sequences.\n",
      "Training on mps\n",
      "Using weighted CrossEntropyLoss\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     mlflow.log_params(hyperparams[model_name])\n\u001b[32m     26\u001b[39m     mlflow.log_param(\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m, model_name) \n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     trained_model = \u001b[43msetup_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#clean up \u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m trained_model \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL/marinetime-project/RNN.py:336\u001b[39m, in \u001b[36msetup_and_train\u001b[39m\u001b[34m(train_df, val_df, test_df, model, hyperparams)\u001b[39m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Placeholder for future model implementation\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# 4. Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeneral\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeneral\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# 5. evalutate on test set\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal evaluation on test set:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL/marinetime-project/RNN.py:148\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate, output_size, class_weights)\u001b[39m\n\u001b[32m    146\u001b[39m optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Learning Rate Scheduler\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m scheduler = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m    151\u001b[39m     total_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "if (config[\"TRAIN_OWN_MODEL\"]):\n",
    "\n",
    "    test_len = len(test_df)\n",
    "    val_len = len(val_df)\n",
    "    train_len = len(train_df)\n",
    "    total_len = test_len + val_len + train_len\n",
    "\n",
    "    #setting hyperparams\n",
    "    params = { \"test_size\": test_len / total_len,\n",
    "        \"val_size\": val_len / total_len,\n",
    "        \"train_split\": train_len / total_len,\n",
    "        }\n",
    "\n",
    "    hyperparams[\"general\"] = {**hyperparams[\"general\"], **params}\n",
    "\n",
    "    mlflow.set_experiment(config[\"Experiment_Name\"])\n",
    "\n",
    "    for model_name in models_to_train:\n",
    "        print(f\"Training model: {model_name}\")\n",
    "        with mlflow.start_run(run_name=f\"{model_name}-Run\"):\n",
    "            #Logging params for later analysis\n",
    "            mlflow.log_params(hyperparams['general'])\n",
    "            mlflow.log_params(hyperparams[model_name])\n",
    "            mlflow.log_param(\"model_type\", model_name) \n",
    "\n",
    "            \n",
    "            trained_model = setup_and_train(train_df=train_df, \n",
    "                                            val_df=val_df, \n",
    "                                            test_df=test_df, \n",
    "                                            hyperparams=hyperparams, \n",
    "                                            model=model_name)\n",
    "\n",
    "        #clean up \n",
    "        del trained_model \n",
    "        gc.collect() \n",
    "        if (torch.cuda.is_available()):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb35e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs found: 3\n",
      "\n",
      "Run 1: 4f4752e38e1048caa87255516d83fc38 - No metrics recorded\n",
      "\n",
      "Run 2: ad0faa9dfcbc44b4ba0d90d84037921b\n",
      "  Status: FINISHED\n",
      "  Metrics: {'train_loss': 1.9527696041833786, 'val_accuracy': 53.164556962025316}\n",
      "\n",
      "Run 3: c1e115e23eb94b438a8a32d137843dc2\n",
      "  Status: FINISHED\n",
      "  Metrics: {'train_loss': 1.9792297874178206, 'val_accuracy': 50.33152501506932}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all runs from the experiment\n",
    "all_runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"start_time DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Total runs found: {len(all_runs)}\\n\")\n",
    "\n",
    "# Go through runs and check which have non-empty metrics\n",
    "for i, run in enumerate(all_runs):\n",
    "    run_id = run.info.run_id\n",
    "    metrics = run.data.metrics\n",
    "    \n",
    "    if metrics:  # Check if metrics dict is not empty\n",
    "        print(f\"Run {i+1}: {run_id}\")\n",
    "        print(f\"  Status: {run.info.status}\")\n",
    "        print(f\"  Metrics: {metrics}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Run {i+1}: {run_id} - No metrics recorded\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d92cda",
   "metadata": {},
   "source": [
    "------------------- RANDOM EDITS BELOW -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7144bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_daily_observations(df, timestamp_col=\"Timestamp\"):\n",
    "    \"\"\"\n",
    "    Groups dataframe by week, counts observations, and plots the time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column (default: \"Timestamp\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Weekly observation counts indexed by week start date\n",
    "    \"\"\"\n",
    "    # Convert to datetime if not already\n",
    "    df_copy = df.copy()\n",
    "    df_copy[timestamp_col] = pd.to_datetime(df_copy[timestamp_col])\n",
    "    \n",
    "    # Extract year-week and count observations per week\n",
    "    df_copy['YearWeek'] = df_copy[timestamp_col].dt.to_period('W')\n",
    "    weekly_counts = df_copy['YearWeek'].value_counts().sort_index()\n",
    "    \n",
    "    # Convert index to datetime for better plotting (use first day of each week)\n",
    "    weekly_counts.index = weekly_counts.index.to_timestamp()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(weekly_counts.index, weekly_counts.values, marker='o', linewidth=1.5, markersize=4)\n",
    "    plt.xlabel('Week', fontsize=12)\n",
    "    plt.ylabel('Number of Observations', fontsize=12)\n",
    "    plt.title('Weekly AIS Observations Over Time', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Total weeks: {len(weekly_counts)}\")\n",
    "    print(f\"Total observations: {weekly_counts.sum():,}\")\n",
    "    print(f\"Average observations per week: {weekly_counts.mean():.0f}\")\n",
    "    print(f\"Min observations in a week: {weekly_counts.min():,}\")\n",
    "    print(f\"Max observations in a week: {weekly_counts.max():,}\")\n",
    "    \n",
    "    return weekly_counts\n",
    "\n",
    "# Use the function\n",
    "daily_obs = plot_daily_observations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a66986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadc771",
   "metadata": {},
   "source": [
    "# Loading models and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e55d8",
   "metadata": {},
   "source": [
    "## Loading run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = MlflowClient()            # or MlflowClient(tracking_uri)\n",
    "experiment_id = \"964646345580301318\"\n",
    "\n",
    "exp = MlflowClient().get_experiment(experiment_id) \n",
    "runs = MlflowClient().search_runs(experiment_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marinetime-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
