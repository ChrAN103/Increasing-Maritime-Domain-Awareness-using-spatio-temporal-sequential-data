{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Dataloader import *\n",
    "import folium\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filepath):\n",
    "    \"\"\"Simple download with progress tracking\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    downloaded_size = 0\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                \n",
    "                # Progress bar\n",
    "                if total_size > 0:\n",
    "                    percent = (downloaded_size / total_size) * 100\n",
    "                    mb_downloaded = downloaded_size / (1024 * 1024)\n",
    "                    mb_total = total_size / (1024 * 1024)\n",
    "                    print(f\"\\rðŸ“¥ Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='')\n",
    "    \n",
    "    print()\n",
    "    print(f\"âœ… Download complete: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd300124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = {\"Jan\": \"http://aisdata.ais.dk/2023/aisdk-2023-01.zip\",\n",
    "              \"Feb\": \"http://aisdata.ais.dk/2023/aisdk-2023-02.zip\",\n",
    "              \"Mar\": \"http://aisdata.ais.dk/2023/aisdk-2023-03.zip\",\n",
    "              \"Apr\": \"http://aisdata.ais.dk/2023/aisdk-2023-04.zip\",\n",
    "              \"May\": \"http://aisdata.ais.dk/2023/aisdk-2023-05.zip\",\n",
    "              \"Jun\": \"http://aisdata.ais.dk/2023/aisdk-2023-06.zip\",\n",
    "              \"Jul\": \"http://aisdata.ais.dk/2023/aisdk-2023-07.zip\",\n",
    "              \"Aug\": \"http://aisdata.ais.dk/2023/aisdk-2023-08.zip\",\n",
    "              \"Sep\": \"http://aisdata.ais.dk/2023/aisdk-2023-09.zip\",\n",
    "              \"Oct\": \"http://aisdata.ais.dk/2023/aisdk-2023-10.zip\",\n",
    "              \"Nov\": \"http://aisdata.ais.dk/2023/aisdk-2023-11.zip\",\n",
    "              \"Dec\": \"http://aisdata.ais.dk/2023/aisdk-2023-12.zip\"}\n",
    "\n",
    "data_dir = \"../data/unprocessed_data\"\n",
    "end_dir = \"../data/processed_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(end_dir, exist_ok=True)\n",
    "\n",
    "for month, link in data_links.items():\n",
    "    filename = link.split(\"/\")[-1]\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Extract month number from filename\n",
    "    month_num = filename.split(\"-\")[2].replace(\".zip\", \"\")\n",
    "\n",
    "    # Check if all parquet files for this month already exist\n",
    "    # Quick check: if folder has files matching this month pattern\n",
    "    existing_parquets = [f for f in os.listdir(end_dir) if f.startswith(f\"aisdk-2023-{month_num}-\") and f.endswith('.parquet')]\n",
    "    \n",
    "    # A month should have 28-31 parquet files\n",
    "    if len(existing_parquets) >= 28:\n",
    "        print(f\"â­ï¸  Skipping month {month}: {len(existing_parquets)} parquet files already exist\")\n",
    "        continue\n",
    "\n",
    "    # Download if needed\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"â­ï¸  Skipping download for month {month}: {filepath} (already exists)\")\n",
    "    else:\n",
    "        print(f\"Downloading data for {month}...\")\n",
    "        download(link, filepath)\n",
    "        print(f\"Downloaded data for {month}\")\n",
    "\n",
    "    # Open ZIP once and process all CSV files\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "        print(f\"Found {len(csv_files)} CSV files in {filename}\")\n",
    "        \n",
    "        for csv_filename in csv_files:\n",
    "            output_filename = csv_filename.replace('.csv', '.parquet')\n",
    "            output_path = os.path.join(end_dir, output_filename)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"â­ï¸  Skipping {output_filename} (already exists)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ðŸ“ Processing: {csv_filename}\")\n",
    "            \n",
    "            data = Dataloader(\n",
    "                file_path=\"\",\n",
    "                out_path=output_path,\n",
    "                zip_path=filepath,\n",
    "                csv_internal_path=csv_filename\n",
    "            )\n",
    "            data.clean_data()\n",
    "\n",
    "    print(f\"âœ… Processed {month}\")\n",
    "\n",
    "    # Remove ZIP after processing\n",
    "    os.remove(filepath)\n",
    "    print(f\"ðŸ—‘ï¸  Removed {filename}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All data processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9ec0b",
   "metadata": {},
   "source": [
    "# Plotting the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "from Dataloader import *\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed_data\"\n",
    "dataloader = Dataloader(out_path=path)\n",
    "# df = dataloader.load_data(date_folders = [\"aisdk-2023-01-01.parquet\"]) # for specific files\n",
    "df = dataloader.load_data()  # load all files in the processed_data folder\n",
    "# Ensure ship and segment can be told apart by adding column for date\n",
    "df['Date'] = df['Timestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f68eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are encountering that a ships sailing over midnight gets split into two segments, even though it's the same journey.\n",
    "# We create a function to overhaul the segment IDs accordingly.\n",
    "# This is happening because in the data cleaning function each day is handled separately, causing segments to reset at midnight.\n",
    "def overhaul_segments(df, time_threshold_minutes=7.5):\n",
    "    \"\"\"\n",
    "    Overhauls segment IDs based on a time threshold between consecutive points.\n",
    "    A new segment is created for a ship if the time gap between two of its\n",
    "    data points is greater than the specified threshold.\n",
    "\n",
    "    input:\n",
    "        df: DataFrame with 'MMSI' and 'Timestamp' columns.\n",
    "        time_threshold_minutes: The maximum time gap in minutes before a new segment is started.\n",
    "    output:\n",
    "        df: DataFrame with a new 'Segment_ID' column.\n",
    "    \"\"\"\n",
    "    # Data is sorted correctly\n",
    "    df = df.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Calculate the time difference between consecutive points for each ship\n",
    "    time_diffs = df.groupby('MMSI')['Timestamp'].diff()\n",
    "\n",
    "    # A new segment starts if the time difference is larger than our threshold\n",
    "    # or if it's the first point for a new ship (where time_diff is NaT)\n",
    "    new_segment_starts = (time_diffs > pd.Timedelta(minutes=time_threshold_minutes)) | (time_diffs.isna())\n",
    "\n",
    "    # Use cumsum() to create a unique, incrementing ID for each segment\n",
    "    df['Segment_ID'] = new_segment_starts.cumsum()\n",
    "\n",
    "    # To make segment IDs restart from 0 for each ship, we can do:\n",
    "    df['Segment_ID'] = df.groupby('MMSI')['Segment_ID'].transform(lambda x: x - x.min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = overhaul_segments(df)\n",
    "df.drop(columns=['Segment'], inplace=True)\n",
    "df.rename(columns={\"Segment_ID\": \"Segment\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e17fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize every vessel's trajectory on a map - RESPECTING SEGMENTS\n",
    "m = folium.Map(location=[55.6761, 12.5683], zoom_start=6)\n",
    "\n",
    "# Plot each vessel's trajectory, with separate polylines for each segment\n",
    "for vessel_id in df[\"MMSI\"].unique()[:int(0.5*len(df[\"MMSI\"].unique()))]: # limit to first 50% of vessels\n",
    "    vessel_data = df[df[\"MMSI\"] == vessel_id]\n",
    "    \n",
    "    # âœ… FIXED: Group by BOTH Segment AND Date\n",
    "    for (segment_id, date), segment in vessel_data.groupby(['Segment', 'Date']):\n",
    "        # Sort by timestamp within the segment\n",
    "        segment = segment.sort_values('Timestamp')\n",
    "        \n",
    "        # Extract coordinates\n",
    "        points = list(zip(segment['Latitude'], segment['Longitude']))\n",
    "        \n",
    "        # Only plot if we have at least 2 points to make a line\n",
    "        if len(points) >= 2:\n",
    "            folium.PolyLine(\n",
    "                locations=points,\n",
    "                color=\"blue\",\n",
    "                weight=2,\n",
    "                opacity=0.6,\n",
    "                popup=f\"Vessel {vessel_id}<br>Segment {segment_id}<br>{len(points)} points\"\n",
    "            ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize every vessel's trajectory on a map - RESPECTING SEGMENTS\n",
    "m = folium.Map(location=[55.6761, 12.5683], zoom_start=6)\n",
    "\n",
    "# Plot each vessel's trajectory, with separate polylines for each segment\n",
    "for vessel_id in df[\"MMSI\"].unique()[int(0.5*len(df[\"MMSI\"].unique())):]: # limit to last 50% of vessels\n",
    "    vessel_data = df[df[\"MMSI\"] == vessel_id]\n",
    "    \n",
    "    # âœ… FIXED: Group by BOTH Segment AND Date\n",
    "    for (segment_id, date), segment in vessel_data.groupby(['Segment', 'Date']):\n",
    "        # Sort by timestamp within the segment\n",
    "        segment = segment.sort_values('Timestamp')\n",
    "        \n",
    "        # Extract coordinates\n",
    "        points = list(zip(segment['Latitude'], segment['Longitude']))\n",
    "        \n",
    "        # Only plot if we have at least 2 points to make a line\n",
    "        if len(points) >= 2:\n",
    "            folium.PolyLine(\n",
    "                locations=points,\n",
    "                color=\"blue\",\n",
    "                weight=2,\n",
    "                opacity=0.6,\n",
    "                popup=f\"Vessel {vessel_id}<br>Segment {segment_id}<br>{len(points)} points\"\n",
    "            ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a772811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
