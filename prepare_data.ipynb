{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f5a2de",
   "metadata": {},
   "source": [
    "# BEWARE THIS FILE TAKES HOURS TO RUN AND DOWNLOADS APPROXIMATELY 200GB (BEFORE COMPRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a439ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Dataloader import *\n",
    "import folium\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfeb4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filepath):\n",
    "    \"\"\"Simple download with progress tracking\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    downloaded_size = 0\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                \n",
    "                # Progress bar\n",
    "                if total_size > 0:\n",
    "                    percent = (downloaded_size / total_size) * 100\n",
    "                    mb_downloaded = downloaded_size / (1024 * 1024)\n",
    "                    mb_total = total_size / (1024 * 1024)\n",
    "                    print(f\"\\rðŸ“¥ Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='')\n",
    "    \n",
    "    print()\n",
    "    print(f\"âœ… Download complete: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd300124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for Jan...\n",
      "ðŸ“¥ Progress: 0.3% (43.7/14834.4 MB)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Open ZIP once and process all CSV files\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, filepath)\u001b[39m\n\u001b[32m      5\u001b[39m downloaded_size = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\site-packages\\urllib3\\response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\site-packages\\urllib3\\response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\site-packages\\urllib3\\response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\http\\client.py:473\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    472\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m s = \u001b[38;5;28mself\u001b[39m.fp.read(amt)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\canic\\miniconda3\\envs\\deep_learn\\Lib\\socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_links = {\"Jan\": \"http://aisdata.ais.dk/2023/aisdk-2023-01.zip\",\n",
    "              \"Feb\": \"http://aisdata.ais.dk/2023/aisdk-2023-02.zip\",\n",
    "              \"Mar\": \"http://aisdata.ais.dk/2023/aisdk-2023-03.zip\",\n",
    "              \"Apr\": \"http://aisdata.ais.dk/2023/aisdk-2023-04.zip\",\n",
    "              \"May\": \"http://aisdata.ais.dk/2023/aisdk-2023-05.zip\",\n",
    "              \"Jun\": \"http://aisdata.ais.dk/2023/aisdk-2023-06.zip\",\n",
    "              \"Jul\": \"http://aisdata.ais.dk/2023/aisdk-2023-07.zip\",\n",
    "              \"Aug\": \"http://aisdata.ais.dk/2023/aisdk-2023-08.zip\",\n",
    "              \"Sep\": \"http://aisdata.ais.dk/2023/aisdk-2023-09.zip\",\n",
    "              \"Oct\": \"http://aisdata.ais.dk/2023/aisdk-2023-10.zip\",\n",
    "              \"Nov\": \"http://aisdata.ais.dk/2023/aisdk-2023-11.zip\",\n",
    "              \"Dec\": \"http://aisdata.ais.dk/2023/aisdk-2023-12.zip\"}\n",
    "\n",
    "data_dir = \"../data/unprocessed_data\"\n",
    "end_dir = \"../data/processed_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(end_dir, exist_ok=True)\n",
    "\n",
    "for month, link in data_links.items():\n",
    "    filename = link.split(\"/\")[-1]\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Extract month number from filename\n",
    "    month_num = filename.split(\"-\")[2].replace(\".zip\", \"\")\n",
    "\n",
    "    # Check if all parquet files for this month already exist\n",
    "    # Quick check: if folder has files matching this month pattern\n",
    "    existing_parquets = [f for f in os.listdir(end_dir) if f.startswith(f\"aisdk-2023-{month_num}-\") and f.endswith('.parquet')]\n",
    "    \n",
    "    # A month should have 28-31 parquet files\n",
    "    if len(existing_parquets) >= 28:\n",
    "        print(f\"â­ï¸  Skipping month {month}: {len(existing_parquets)} parquet files already exist\")\n",
    "        continue\n",
    "\n",
    "    # Download if needed\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"â­ï¸  Skipping download for month {month}: {filepath} (already exists)\")\n",
    "    else:\n",
    "        print(f\"Downloading data for {month}...\")\n",
    "        download(link, filepath)\n",
    "        print(f\"Downloaded data for {month}\")\n",
    "\n",
    "    # Open ZIP once and process all CSV files\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]\n",
    "        print(f\"Found {len(csv_files)} CSV files in {filename}\")\n",
    "        \n",
    "        for csv_filename in csv_files:\n",
    "            output_filename = csv_filename.replace('.csv', '.parquet')\n",
    "            output_path = os.path.join(end_dir, output_filename)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"â­ï¸  Skipping {output_filename} (already exists)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ðŸ“ Processing: {csv_filename}\")\n",
    "            \n",
    "            data = Dataloader(\n",
    "                file_path=\"\",\n",
    "                out_path=output_path,\n",
    "                zip_path=filepath,\n",
    "                csv_internal_path=csv_filename\n",
    "            )\n",
    "            data.clean_data()\n",
    "\n",
    "    print(f\"âœ… Processed {month}\")\n",
    "\n",
    "    # Remove ZIP after processing\n",
    "    os.remove(filepath)\n",
    "    print(f\"ðŸ—‘ï¸  Removed {filename}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All data processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(os.path.join(end_dir, \"aisdk-2023-02-11.parquet\"))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6538b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize every vessel's trajectory on a map - RESPECTING SEGMENTS\n",
    "# m = folium.Map(location=[55.6761, 12.5683], zoom_start=6)\n",
    "\n",
    "# # Plot each vessel's trajectory, with separate polylines for each segment\n",
    "# for vessel_id in df[\"MMSI\"].unique():\n",
    "#     vessel_data = df[df[\"MMSI\"] == vessel_id]\n",
    "    \n",
    "#     # Group by segment and plot each segment separately\n",
    "#     for segment_id, segment in vessel_data.groupby('Segment'):\n",
    "#         # Sort by timestamp within the segment\n",
    "#         segment = segment.sort_values('Timestamp')\n",
    "        \n",
    "#         # Extract coordinates\n",
    "#         points = list(zip(segment['Latitude'], segment['Longitude']))\n",
    "        \n",
    "#         # Only plot if we have at least 2 points to make a line\n",
    "#         if len(points) >= 2:\n",
    "#             folium.PolyLine(\n",
    "#                 locations=points,\n",
    "#                 color=\"blue\",\n",
    "#                 weight=2,\n",
    "#                 opacity=0.6,\n",
    "#                 popup=f\"Vessel {vessel_id}<br>Segment {segment_id}<br>{len(points)} points\"\n",
    "#             ).add_to(m)\n",
    "\n",
    "# m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
